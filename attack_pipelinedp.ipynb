{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx4EVNyulFQ3"
      },
      "source": [
        "## Attack idea from April 4\n",
        "\n",
        "*The text below is from Salil's email describing the attack*\n",
        "\n",
        "\n",
        "How about the following datasets.  Let n and U be powers of 2, and m=n/2.\n",
        "\n",
        "u = m copies of U, followed by m/2 copies of alternating the values (U * m / 2^k) * (1 / 2 + 1 / 2^k) and -(U * m / 2^k) * (1 / 2 - 1 / 2^k)\n",
        "\n",
        "If I'm correct, then since ULP(U * m) = U * m / 2^k, and when we're doing the alternation, all the rounding will be upward and BS^*(u) will be m * U + (m / 2) * (U * m / 2^k). \n",
        "\n",
        "v = same, but one less copy of U.\n",
        "\n",
        "Then since ULP(U*(m-1)) <= (U * m / 2^k) * (1 / 2), when we're doing the alternation, we alternately round up and round down, so BS^* (v) = m * U\n",
        "\n",
        "Overall, BS^* (u) - BS^* (v) = (m/2) * (U * m) / 2^k = U * (n^2) / (2^{k+3}).\n",
        "\n",
        "So we get a blow-up in sensitivity by a factor of n^2/(2^{k+3}).  The key point is that we have an n^2 rather than an n in the numerator.  For example, taking n=2^{30}, we get a blow-up of 32 in the sensitivity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from generate_datasets import unsized_64_consts, unsized_alternating_values\n",
        "consts = unsized_64_consts()\n",
        "U = consts['U']\n",
        "m = consts['m']\n",
        "neg_val, pos_val = unsized_alternating_values(consts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PipelineDP Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set up pipelinedp\n",
        "import pipeline_dp\n",
        "from pipeline_dp import SumParams\n",
        "from pipeline_dp.private_spark import make_private\n",
        "import pyspark\n",
        "import uuid\n",
        "arr_u_file = 'arr_u.csv'\n",
        "arr_v_file = 'arr_v.csv'\n",
        "\n",
        "\n",
        "# to demonstrate this attack, we want to add no noise\n",
        "def make_inf_accountant():\n",
        "    # crashes opaquely if you pass `float('inf')`, so just set epsilon to a large value\n",
        "    return pipeline_dp.NaiveBudgetAccountant(total_epsilon=1e20, total_delta=1.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set up pyspark\n",
        "import os\n",
        "os.environ['JAVA_HOME'] = \"/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home\"\n",
        "\n",
        "import pyspark\n",
        "sc = pyspark.SparkContext()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute aggregates directly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "134217727.99999999"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# pyspark sums are less susceptible\n",
        "sc.textFile(arr_u_file).map(float).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "134217727.0"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sc.textFile(arr_v_file).map(float).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute aggregates with PipelineDP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0, 134217728.0)]"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 110 minutes\n",
        "from pipeline_dp.private_spark import make_private\n",
        "\n",
        "# when we compute a private sum, use laplace noise without partitioning\n",
        "sum_params = SumParams(\n",
        "    noise_kind=NoiseKind.LAPLACE,\n",
        "    max_partitions_contributed=1,\n",
        "    max_contributions_per_partition=1,\n",
        "    min_value=neg_val,\n",
        "    max_value=1,\n",
        "    public_partitions=[0],\n",
        "    partition_extractor=lambda _: 0,\n",
        "    value_extractor=lambda v: v)\n",
        "\n",
        "arr_u_rdd = sc.textFile(arr_u_file).map(float)\n",
        "\n",
        "accountant = make_inf_accountant()\n",
        "# Wrap Spark's RDD into its private version\n",
        "# assign a random uuid user id to each row, to represent that each row is unique\n",
        "private_arr_u_rdd = make_private(arr_u_rdd, accountant, lambda _: str(uuid.uuid4()))\n",
        "\n",
        "# Calculate the private sum\n",
        "dp_result = private_arr_u_rdd.sum(sum_params)\n",
        "accountant.compute_budgets()\n",
        "\n",
        "# not vulnerable, because the implementation uses the spark summer, which has previously shown to be robust\n",
        "dp_result.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Beam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n"
          ]
        }
      ],
      "source": [
        "import apache_beam as beam\n",
        "from apache_beam.runners.portability import fn_api_runner\n",
        "\n",
        "runner = fn_api_runner.FnApiRunner()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute aggregates directly. Shows that beam sum is susceptible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "134217730.0\n"
          ]
        }
      ],
      "source": [
        "# 53 minutes\n",
        "with beam.Pipeline(runner=runner) as pipeline:\n",
        "    pipeline \\\n",
        "        | beam.io.ReadFromText(arr_u_file) \\\n",
        "        | beam.Map(float) \\\n",
        "        | beam.CombineGlobally(sum) \\\n",
        "        | beam.Map(print)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute aggregates with PipelineDP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pipeline_dp.private_beam import MakePrivate\n",
        "from pipeline_dp.aggregate_params import NoiseKind\n",
        "from pipeline_dp import private_beam, SumParams\n",
        "import pipeline_dp\n",
        "\n",
        "# crashes opaquely if public_partitions are set\n",
        "sum_params = SumParams(\n",
        "    noise_kind=NoiseKind.LAPLACE,\n",
        "    max_partitions_contributed=1,\n",
        "    max_contributions_per_partition=1,\n",
        "    min_value=neg_val,\n",
        "    max_value=1,\n",
        "    # public_partitions=[0],\n",
        "    partition_extractor=lambda _: 0,\n",
        "    value_extractor=lambda v: v[1])\n",
        "\n",
        "total_len = m + 2**27\n",
        "\n",
        "with beam.Pipeline(runner=runner) as pipeline:\n",
        "    accountant = make_inf_accountant()\n",
        "\n",
        "    def parse_line(l):\n",
        "        i, v = l.split(',')\n",
        "        return int(i), float(v)\n",
        "\n",
        "    def attack_transform(row):\n",
        "        i, v = row\n",
        "        if i < m:\n",
        "            return i, U\n",
        "        if i == m:\n",
        "            return i, U * v\n",
        "        if i % 2 == 1:\n",
        "            return i, neg_val\n",
        "        return i, pos_val\n",
        "    \n",
        "    # Load and parse input data\n",
        "    dp_result = pipeline \\\n",
        "        | beam.io.ReadFromText('arr_u_i.csv') \\\n",
        "        | beam.Map(parse_line) \\\n",
        "        | MakePrivate(\n",
        "            budget_accountant=accountant,\n",
        "            privacy_id_extractor=lambda r: r[0]) \\\n",
        "        | private_beam.Map(attack_transform) \\\n",
        "        | private_beam.Sum(sum_params)\n",
        "\n",
        "    accountant.compute_budgets()\n",
        "\n",
        "    dp_result | beam.Map(print)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "attack_pipelinedp.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
